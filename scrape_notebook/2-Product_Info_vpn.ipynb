{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation: Given the product link, scape the product info we need.\n",
    "    - product info includes product name, product id, price, size, love counts, review counts, etc...\n",
    "    - Save the product info as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(product_link):\n",
    "    \"\"\"\n",
    "    Given a product link, return a dictionary of product info \n",
    "    including product id, product name, brand, category, item \n",
    "    number, price, size, love counts, review counts, rating and link\n",
    "    \"\"\"\n",
    "    response = requests.get(product_link)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    product_id = re.findall(R'P[0-9]{3,6}', product_link)[0]\n",
    "\n",
    "    # Some product may have been removed.\n",
    "    if soup.find('h1', class_='css-56434t'):\n",
    "        return None\n",
    "\n",
    "    # Get brand name and product name\n",
    "    for brand_and_name in soup.find_all('h1', class_='css-a1jw00'):\n",
    "        names = [names.get_text() for names in brand_and_name.find_all('span')]\n",
    "        brand = names[0]\n",
    "        prd_name = names[1]\n",
    "\n",
    "    # Get Category and price (use json to get categories and price),\n",
    "    # Because it would give us sub categories and offer price.\n",
    "    # But if that doesn't work, just get main category and price box.\n",
    "    dic_json = soup.find_all(attrs={\"type\": \"application/ld+json\"})\n",
    "    try:\n",
    "        json_category = json.loads(dic_json[0].get_text())\n",
    "        price = json.loads(dic_json[1].get_text())['offers'][0]['price']\n",
    "        category_lst = [item['item']['name']\n",
    "                        for item in json_category['itemListElement']]\n",
    "        category = '/'.join(category_lst)\n",
    "\n",
    "    except:\n",
    "        try:\n",
    "            category = [cate.get_text()\n",
    "                        for cate in soup.find_all('ol', class_='css-1doqpel')]\n",
    "            price = soup.find(\n",
    "                'div', attrs={\"data-comp\": \"Price Box\"}).get_text()\n",
    "        except:\n",
    "            price = 'na'\n",
    "            category = 'na'\n",
    "\n",
    "    # Get item number and size\n",
    "    size_and_item = soup.find(attrs={\"data-comp\": \"SizeAndItemNumber Box\"})\n",
    "    # If there's only item number and no size info, it will be out of index\n",
    "    try:\n",
    "        item_num = size_and_item.contents[1].split(' ')[1]\n",
    "    except:\n",
    "        item_num = size_and_item.contents[0].split(' ')[1]\n",
    "    # If there's no size at SizeAndItemNumber Box, then try use description area to see.\n",
    "    try:\n",
    "        size = size_and_item.span.contents[0].split('SIZE ')[1]\n",
    "    except:\n",
    "        try:\n",
    "            size = soup.find(\n",
    "                \"span\", attrs={\"class\": \"css-12wl10d\"}).contents[-1]\n",
    "        except:\n",
    "            size = 'na'\n",
    "\n",
    "    # Get love counts\n",
    "    try:\n",
    "        love_counts = soup.find(\n",
    "            'span', attrs={\"data-at\": \"product_love_count\"}).get_text()\n",
    "    except:\n",
    "        love_counts = 'na'\n",
    "\n",
    "    # review nums and ratings\n",
    "    link_json = soup.find(attrs={\"id\": \"linkJSON\"})\n",
    "    json_str = str(link_json)\n",
    "    ratings = re.findall(R'\\\"rating\\\"\\:(.*?)\\,', json_str)\n",
    "    reviews = re.findall(R'\\\"reviews\\\"\\:(.*?)\\,', json_str)\n",
    "    try:\n",
    "        rating = ratings[0]\n",
    "    except:\n",
    "        rating = 'na'\n",
    "    try:\n",
    "        reviews_count = reviews[0]\n",
    "    except:\n",
    "        reviews_count = 'na'\n",
    "\n",
    "    dic1 = {}\n",
    "    dic1['Product_Id'] = product_id\n",
    "    dic1['product_name'] = prd_name\n",
    "    dic1['item_num'] = item_num\n",
    "    dic1['brand'] = brand\n",
    "    dic1['category'] = category\n",
    "    dic1['price'] = price\n",
    "    dic1['size'] = size\n",
    "    dic1['love_count'] = love_counts\n",
    "    dic1['rating'] = rating\n",
    "    dic1['reviews_count'] = reviews_count\n",
    "    dic1['link'] = product_link\n",
    "    return dic1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: get first 5 product info\n",
    "products = open('product_link.txt')\n",
    "i = 0\n",
    "frame = []\n",
    "for item in products:\n",
    "    link = item.rstrip()\n",
    "    product_data = get_data(link)\n",
    "    if product_data:\n",
    "        df = pd.DataFrame(product_data,index=[i])\n",
    "        frame.append(df)\n",
    "        i+=1\n",
    "        time.sleep(np.random.random()*3)\n",
    "    if i == 5:\n",
    "        break\n",
    "result = pd.concat(frame)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many products? \n",
    "products = open('product_link.txt')\n",
    "count = 0\n",
    "for item in products:\n",
    "    count += 1\n",
    "print('Product counts: ', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are over 3000 products on Sephora website. Therefore, to prevent blocking, here we have two ways: try different vpn or change proxies.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scape product info with different vpn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First time: use vpn to scape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1st time: scape first 1500 products.\n",
    "products = open('product_link.txt')\n",
    "product_links = []\n",
    "for item in products:\n",
    "    link = item.rstrip()\n",
    "    product_links.append(link)\n",
    "    \n",
    "frame1 = []\n",
    "i = 0\n",
    "for link in product_links[:1500]: \n",
    "    print(i, link)\n",
    "    product_data = get_data(link)\n",
    "    if product_data:\n",
    "        df = pd.DataFrame(product_data,index=[i])\n",
    "        frame1.append(df)\n",
    "        i+=1\n",
    "        time.sleep(np.random.random()*3)\n",
    "\n",
    "result1 = pd.concat(frame1)\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1.to_csv('result1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second time: change the vpn and run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2nd time: scape the left products.\n",
    "frame2 = []\n",
    "i = 1500 \n",
    "for link in product_links[1500:]: \n",
    "    print(i, link)\n",
    "    product_data = get_data(link)\n",
    "    if product_data:\n",
    "        df = pd.DataFrame(product_data,index=[i])\n",
    "        frame2.append(df)\n",
    "        i+=1\n",
    "        time.sleep(np.random.random()*3)\n",
    "result2 = pd.concat(frame2)\n",
    "result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Combine the results above and get the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([result1,result2])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the result as csv. \n",
    "result.to_csv('result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
